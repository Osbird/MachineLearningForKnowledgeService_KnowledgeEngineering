{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3_20186484_Osberg.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "xWbdcfV62Fhd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Recommended runtime environment is using [Google Colaboratory](https://colab.research.google.com/)"
      ]
    },
    {
      "metadata": {
        "id": "0BMFxTMWxPL0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U nltk #adapted for google colab or linux environments\n",
        "\n",
        "import urllib #URL handling library\n",
        "import nltk\n",
        "nltk.download('all') \n",
        "# 'all' is all packages\n",
        "# 'book' is everything used in the NLTK Book\n",
        "# 'popular' uses most useful/popular packages\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7R0_y1nfWgxG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "textFileURL = 'http://www.gutenberg.org/cache/epub/5200/pg5200.txt'\n",
        "\n",
        "# load text from URL, for a swift local environment\n",
        "data = urllib.request.urlopen(textFileURL) #returns object that can be read like a file\n",
        "book = data.read()\n",
        "data.close()\n",
        "\n",
        "# Convert from byteArray to string\n",
        "book = book.decode('utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R2eRQzgGU_Fl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cleanText(text):\n",
        "  # split into words/tokens\n",
        "  from nltk import word_tokenize\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # set to lower case\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "\n",
        "  # stem words\n",
        "  from nltk.stem.porter import PorterStemmer\n",
        "  porter = PorterStemmer()\n",
        "  stemmed = [porter.stem(word) for word in tokens]\n",
        "\n",
        "  # rm puctuation\n",
        "  import string\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  stripped = [w.translate(table) for w in stemmed]\n",
        "\n",
        "  # remove non-aplhabetic charachters\n",
        "  words = [word for word in stripped if word.isalpha()]\n",
        "  return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mG-W2XphTFKO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creation of our own stop word list using wikipedia as our document databse\n",
        "Done by calculating the IDF wights for each word. This is dicussed as an approach to find stop words in [this paper](http://terrierteam.dcs.gla.ac.uk/publications/rtlo_DIRpaper.pdf)\n",
        "\n",
        "Downloaded by the [wikipedia package](https://pypi.org/project/wikipedia/)\n",
        "\n",
        "Below are the experiments using the entire [Gutenberg dataset](https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html)(at 1.2 GB of text) as our document base for creating a stop word list. The results from this exoeriment are described below"
      ]
    },
    {
      "metadata": {
        "id": "H0Mw09rXyLnY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# install the wikipedia package. This is adapter for google colab or any other linux enviroment\n",
        "!pip install -U wikipedia\n",
        "import wikipedia"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ytFlN9DgzbVi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# article search results for the following terms. Can easily search based on random terms\n",
        "searchTermList = ['norway','south korea','cherry blossom','strawberry','easter','norse mythology','silla era','shakespeare','italian cooking','google','planck constant','higgs boson']\n",
        "articleNamesTotal = []\n",
        "for elem in searchTermList:\n",
        "  articleNames = wikipedia.search(elem)\n",
        "  articleNamesTotal = articleNamesTotal+articleNames\n",
        "\n",
        "# add each article to document dictionary\n",
        "textsList = []\n",
        "for elem in (articleNamesTotal):\n",
        "  try:\n",
        "    # get document text and tokenize it\n",
        "    text = wikipedia.page(elem).content\n",
        "    textsList.append(cleanText(text))\n",
        "  except wikipedia.exceptions.DisambiguationError as e:\n",
        "    print(elem + \" - raised a dismbiguationError as it may refer to multiple pages\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vAYtRMyI4Fdt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4012
        },
        "outputId": "f35412f1-92d0-4b58-e6be-389d3f544434"
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "from itertools import chain\n",
        "\n",
        "# compute bottom IDF words of given percentage of total. This should constitute our stop word list\n",
        "def computeBottomIDF(textsList, percentage):\n",
        "  flattenedList = list(chain.from_iterable(textsList))\n",
        "  \n",
        "  # create dictionary of word:idf-weight\n",
        "  scores = {word: idf(word,textsList) for word in set(flattenedList)}\n",
        "  \n",
        "  # sort total & take percentage of elements\n",
        "  sortedWords = sorted(scores.items(), key=lambda x: x[1], reverse = False)\n",
        "  percentageOf_sortedWords = sortedWords[:math.floor(len(sortedWords)*percentage)]\n",
        "  \n",
        "  printWordAndScore(percentageOf_sortedWords, 'IDF')\n",
        "  return [word[0] for word in percentageOf_sortedWords]\n",
        "\n",
        "# print word & score to console in handy format\n",
        "def printWordAndScore(sortedTokenList, scoreType):\n",
        "  for word, score in sortedTokenList:\n",
        "    print(\"  Word: {},\\t {}: {}\".format(word, scoreType, round(score, 5)).expandtabs(30))\n",
        "    \n",
        "# IR elementary functions\n",
        "def tf(word, text):\n",
        "  return text.count(word) / len(text)\n",
        "\n",
        "def n_containing(word, textsList):\n",
        "    return sum(1 for text in textsList if word in text)\n",
        "\n",
        "def idf(word, textsList):\n",
        "    return math.log(len(textsList) / (n_containing(word, textsList)))\n",
        "\n",
        "def tfidf(word, text, textsList):\n",
        "    return tf(word, text) * idf(word, textsList)\n",
        "\n",
        "privateGeneratedStopWordList = computeBottomIDF(textsList, 0.01)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Word: the,                   IDF: 0.0\n",
            "  Word: as,                    IDF: 0.0\n",
            "  Word: in,                    IDF: 0.0\n",
            "  Word: and,                   IDF: 0.00873\n",
            "  Word: of,                    IDF: 0.00873\n",
            "  Word: refer,                 IDF: 0.00873\n",
            "  Word: a,                     IDF: 0.00873\n",
            "  Word: to,                    IDF: 0.02643\n",
            "  Word: for,                   IDF: 0.02643\n",
            "  Word: is,                    IDF: 0.0354\n",
            "  Word: it,                    IDF: 0.0354\n",
            "  Word: with,                  IDF: 0.04445\n",
            "  Word: by,                    IDF: 0.04445\n",
            "  Word: also,                  IDF: 0.04445\n",
            "  Word: which,                 IDF: 0.0628\n",
            "  Word: that,                  IDF: 0.0628\n",
            "  Word: from,                  IDF: 0.0721\n",
            "  Word: are,                   IDF: 0.08149\n",
            "  Word: on,                    IDF: 0.09097\n",
            "  Word: an,                    IDF: 0.10054\n",
            "  Word: thi,                   IDF: 0.10054\n",
            "  Word: or,                    IDF: 0.1102\n",
            "  Word: wa,                    IDF: 0.11996\n",
            "  Word: see,                   IDF: 0.11996\n",
            "  Word: be,                    IDF: 0.12981\n",
            "  Word: s,                     IDF: 0.12981\n",
            "  Word: use,                   IDF: 0.12981\n",
            "  Word: have,                  IDF: 0.12981\n",
            "  Word: at,                    IDF: 0.13976\n",
            "  Word: other,                 IDF: 0.14981\n",
            "  Word: but,                   IDF: 0.14981\n",
            "  Word: one,                   IDF: 0.15996\n",
            "  Word: their,                 IDF: 0.18058\n",
            "  Word: link,                  IDF: 0.18058\n",
            "  Word: ha,                    IDF: 0.18058\n",
            "  Word: all,                   IDF: 0.19106\n",
            "  Word: into,                  IDF: 0.19106\n",
            "  Word: such,                  IDF: 0.19106\n",
            "  Word: time,                  IDF: 0.19106\n",
            "  Word: not,                   IDF: 0.20164\n",
            "  Word: extern,                IDF: 0.21233\n",
            "  Word: first,                 IDF: 0.21233\n",
            "  Word: includ,                IDF: 0.21233\n",
            "  Word: when,                  IDF: 0.21233\n",
            "  Word: were,                  IDF: 0.21233\n",
            "  Word: some,                  IDF: 0.23407\n",
            "  Word: mani,                  IDF: 0.23407\n",
            "  Word: there,                 IDF: 0.23407\n",
            "  Word: onli,                  IDF: 0.24512\n",
            "  Word: these,                 IDF: 0.24512\n",
            "  Word: two,                   IDF: 0.24512\n",
            "  Word: between,               IDF: 0.2563\n",
            "  Word: been,                  IDF: 0.2563\n",
            "  Word: had,                   IDF: 0.2563\n",
            "  Word: most,                  IDF: 0.27902\n",
            "  Word: who,                   IDF: 0.27902\n",
            "  Word: more,                  IDF: 0.29058\n",
            "  Word: state,                 IDF: 0.29058\n",
            "  Word: they,                  IDF: 0.29058\n",
            "  Word: known,                 IDF: 0.29058\n",
            "  Word: howev,                 IDF: 0.30228\n",
            "  Word: than,                  IDF: 0.30228\n",
            "  Word: new,                   IDF: 0.30228\n",
            "  Word: would,                 IDF: 0.31412\n",
            "  Word: over,                  IDF: 0.32609\n",
            "  Word: after,                 IDF: 0.32609\n",
            "  Word: them,                  IDF: 0.33821\n",
            "  Word: name,                  IDF: 0.33821\n",
            "  Word: while,                 IDF: 0.33821\n",
            "  Word: made,                  IDF: 0.33821\n",
            "  Word: where,                 IDF: 0.35048\n",
            "  Word: no,                    IDF: 0.35048\n",
            "  Word: about,                 IDF: 0.35048\n",
            "  Word: can,                   IDF: 0.36291\n",
            "  Word: origin,                IDF: 0.36291\n",
            "  Word: make,                  IDF: 0.36291\n",
            "  Word: later,                 IDF: 0.36291\n",
            "  Word: year,                  IDF: 0.36291\n",
            "  Word: same,                  IDF: 0.37548\n",
            "  Word: three,                 IDF: 0.37548\n",
            "  Word: gener,                 IDF: 0.37548\n",
            "  Word: sinc,                  IDF: 0.37548\n",
            "  Word: follow,                IDF: 0.37548\n",
            "  Word: then,                  IDF: 0.38822\n",
            "  Word: histori,               IDF: 0.38822\n",
            "  Word: well,                  IDF: 0.38822\n",
            "  Word: differ,                IDF: 0.38822\n",
            "  Word: up,                    IDF: 0.40113\n",
            "  Word: accord,                IDF: 0.40113\n",
            "  Word: call,                  IDF: 0.40113\n",
            "  Word: part,                  IDF: 0.40113\n",
            "  Word: sever,                 IDF: 0.40113\n",
            "  Word: like,                  IDF: 0.40113\n",
            "  Word: until,                 IDF: 0.40113\n",
            "  Word: dure,                  IDF: 0.4142\n",
            "  Word: both,                  IDF: 0.4142\n",
            "  Word: list,                  IDF: 0.4142\n",
            "  Word: world,                 IDF: 0.4142\n",
            "  Word: number,                IDF: 0.4142\n",
            "  Word: may,                   IDF: 0.42744\n",
            "  Word: second,                IDF: 0.42744\n",
            "  Word: each,                  IDF: 0.42744\n",
            "  Word: so,                    IDF: 0.44087\n",
            "  Word: hi,                    IDF: 0.45447\n",
            "  Word: around,                IDF: 0.45447\n",
            "  Word: although,              IDF: 0.45447\n",
            "  Word: base,                  IDF: 0.45447\n",
            "  Word: becaus,                IDF: 0.45447\n",
            "  Word: creat,                 IDF: 0.48225\n",
            "  Word: earli,                 IDF: 0.48225\n",
            "  Word: becam,                 IDF: 0.49644\n",
            "  Word: still,                 IDF: 0.49644\n",
            "  Word: out,                   IDF: 0.49644\n",
            "  Word: unit,                  IDF: 0.49644\n",
            "  Word: form,                  IDF: 0.49644\n",
            "  Word: larg,                  IDF: 0.49644\n",
            "  Word: result,                IDF: 0.51083\n",
            "  Word: similar,               IDF: 0.51083\n",
            "  Word: befor,                 IDF: 0.51083\n",
            "  Word: found,                 IDF: 0.51083\n",
            "  Word: univers,               IDF: 0.51083\n",
            "  Word: consid,                IDF: 0.52542\n",
            "  Word: mean,                  IDF: 0.52542\n",
            "  Word: remain,                IDF: 0.52542\n",
            "  Word: appear,                IDF: 0.52542\n",
            "  Word: through,               IDF: 0.52542\n",
            "  Word: ani,                   IDF: 0.52542\n",
            "  Word: end,                   IDF: 0.52542\n",
            "  Word: if,                    IDF: 0.54024\n",
            "  Word: way,                   IDF: 0.54024\n",
            "  Word: work,                  IDF: 0.54024\n",
            "  Word: did,                   IDF: 0.55528\n",
            "  Word: nation,                IDF: 0.55528\n",
            "  Word: note,                  IDF: 0.55528\n",
            "  Word: anoth,                 IDF: 0.55528\n",
            "  Word: veri,                  IDF: 0.55528\n",
            "  Word: further,               IDF: 0.55528\n",
            "  Word: provid,                IDF: 0.55528\n",
            "  Word: will,                  IDF: 0.57054\n",
            "  Word: addit,                 IDF: 0.57054\n",
            "  Word: take,                  IDF: 0.57054\n",
            "  Word: under,                 IDF: 0.57054\n",
            "  Word: peopl,                 IDF: 0.57054\n",
            "  Word: those,                 IDF: 0.57054\n",
            "  Word: he,                    IDF: 0.57054\n",
            "  Word: often,                 IDF: 0.57054\n",
            "  Word: order,                 IDF: 0.58605\n",
            "  Word: even,                  IDF: 0.58605\n",
            "  Word: continu,               IDF: 0.58605\n",
            "  Word: day,                   IDF: 0.58605\n",
            "  Word: late,                  IDF: 0.6018\n",
            "  Word: relat,                 IDF: 0.6018\n",
            "  Word: develop,               IDF: 0.6018\n",
            "  Word: place,                 IDF: 0.6018\n",
            "  Word: do,                    IDF: 0.6018\n",
            "  Word: among,                 IDF: 0.6018\n",
            "  Word: popular,               IDF: 0.6178\n",
            "  Word: becom,                 IDF: 0.6178\n",
            "  Word: contain,               IDF: 0.6178\n",
            "  Word: much,                  IDF: 0.6178\n",
            "  Word: present,               IDF: 0.6178\n",
            "  Word: tradit,                IDF: 0.6178\n",
            "  Word: own,                   IDF: 0.63406\n",
            "  Word: import,                IDF: 0.63406\n",
            "  Word: variou,                IDF: 0.63406\n",
            "  Word: small,                 IDF: 0.63406\n",
            "  Word: now,                   IDF: 0.63406\n",
            "  Word: down,                  IDF: 0.63406\n",
            "  Word: four,                  IDF: 0.63406\n",
            "  Word: start,                 IDF: 0.63406\n",
            "  Word: due,                   IDF: 0.65059\n",
            "  Word: natur,                 IDF: 0.65059\n",
            "  Word: term,                  IDF: 0.65059\n",
            "  Word: last,                  IDF: 0.65059\n",
            "  Word: describ,               IDF: 0.65059\n",
            "  Word: countri,               IDF: 0.65059\n",
            "  Word: within,                IDF: 0.66739\n",
            "  Word: though,                IDF: 0.66739\n",
            "  Word: could,                 IDF: 0.66739\n",
            "  Word: period,                IDF: 0.66739\n",
            "  Word: produc,                IDF: 0.66739\n",
            "  Word: group,                 IDF: 0.66739\n",
            "  Word: show,                  IDF: 0.66739\n",
            "  Word: given,                 IDF: 0.68449\n",
            "  Word: major,                 IDF: 0.68449\n",
            "  Word: consist,               IDF: 0.68449\n",
            "  Word: exampl,                IDF: 0.68449\n",
            "  Word: introduc,              IDF: 0.70188\n",
            "  Word: give,                  IDF: 0.70188\n",
            "  Word: area,                  IDF: 0.70188\n",
            "  Word: cultur,                IDF: 0.70188\n",
            "  Word: chang,                 IDF: 0.70188\n",
            "  Word: exist,                 IDF: 0.70188\n",
            "  Word: set,                   IDF: 0.70188\n",
            "  Word: product,               IDF: 0.70188\n",
            "  Word: common,                IDF: 0.71958\n",
            "  Word: close,                 IDF: 0.71958\n",
            "  Word: begin,                 IDF: 0.71958\n",
            "  Word: togeth,                IDF: 0.71958\n",
            "  Word: great,                 IDF: 0.7376\n",
            "  Word: usual,                 IDF: 0.7376\n",
            "  Word: live,                  IDF: 0.7376\n",
            "  Word: effect,                IDF: 0.7376\n",
            "  Word: sourc,                 IDF: 0.7376\n",
            "  Word: public,                IDF: 0.7376\n",
            "  Word: long,                  IDF: 0.7376\n",
            "  Word: centuri,               IDF: 0.7376\n",
            "  Word: without,               IDF: 0.7376\n",
            "  Word: high,                  IDF: 0.7376\n",
            "  Word: role,                  IDF: 0.7376\n",
            "  Word: north,                 IDF: 0.7376\n",
            "  Word: began,                 IDF: 0.7376\n",
            "  Word: what,                  IDF: 0.7376\n",
            "  Word: against,               IDF: 0.75595\n",
            "  Word: local,                 IDF: 0.75595\n",
            "  Word: rather,                IDF: 0.75595\n",
            "  Word: suggest,               IDF: 0.75595\n",
            "  Word: lead,                  IDF: 0.75595\n",
            "  Word: power,                 IDF: 0.75595\n",
            "  Word: design,                IDF: 0.75595\n",
            "  Word: come,                  IDF: 0.75595\n",
            "  Word: establish,             IDF: 0.75595\n",
            "  Word: system,                IDF: 0.77464\n",
            "  Word: along,                 IDF: 0.77464\n",
            "  Word: support,               IDF: 0.77464\n",
            "  Word: type,                  IDF: 0.77464\n",
            "  Word: combin,                IDF: 0.77464\n",
            "  Word: offici,                IDF: 0.77464\n",
            "  Word: possibl,               IDF: 0.77464\n",
            "  Word: american,              IDF: 0.77464\n",
            "  Word: point,                 IDF: 0.77464\n",
            "  Word: associ,                IDF: 0.77464\n",
            "  Word: book,                  IDF: 0.77464\n",
            "  Word: play,                  IDF: 0.77464\n",
            "  Word: led,                   IDF: 0.79369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XOHMcCRx4Fh-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48c8c021-ba9b-4c80-fa18-dcb56ca187ae"
      },
      "cell_type": "code",
      "source": [
        "# text cleaning up until tokenization\n",
        "words = cleanText(book)\n",
        "\n",
        "# fetch general english stop word list\n",
        "from nltk.corpus import stopwords\n",
        "stop_words_general = stopwords.words('english')\n",
        "print('length of general stop word list: {}  -  private stop word list: {}'.format(len(stop_words_general),len(privateGeneratedStopWordList)))\n",
        "\n",
        "# remove stop words from either list\n",
        "wordsWithoutGeneralStopWords = [word for word in words if not word in stop_words_general]\n",
        "wordsWithoutPrivateStopWords = [word for word in words if not word in privateGeneratedStopWordList]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of general stop word list: 179  -  private stop word list: 235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D-YvSiCg4FcL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# compute top x TF words of text  \n",
        "def printTopTF(text, amount):\n",
        "  # create dict of tf per unique word in text \n",
        "  scores = {word: tf(word,text) for word in set(text)}\n",
        "  \n",
        "  # sort total & take top <amount> of elements\n",
        "  sortedWords = sorted(scores.items(), key=lambda x: x[1], reverse = True)\n",
        "  amountOf_sortedWords = sortedWords[:amount]\n",
        "  \n",
        "  # helper function for handy print format\n",
        "  printWordAndScore(amountOf_sortedWords, 'TF')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "en-BDsCGgACH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "0f2f82e8-ec0c-4177-a913-b935f7c5e717"
      },
      "cell_type": "code",
      "source": [
        "# Calculate tf scores for top 50 words from wordsWithoutGeneralStopWords token sets\n",
        "printTopTF(wordsWithoutGeneralStopWords, 50)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Word: hi,                    TF: 0.04174\n",
            "  Word: wa,                    TF: 0.03142\n",
            "  Word: gregor,                TF: 0.02262\n",
            "  Word: would,                 TF: 0.01449\n",
            "  Word: thi,                   TF: 0.011\n",
            "  Word: room,                  TF: 0.01009\n",
            "  Word: could,                 TF: 0.00964\n",
            "  Word: work,                  TF: 0.00865\n",
            "  Word: even,                  TF: 0.00789\n",
            "  Word: father,                TF: 0.00774\n",
            "  Word: sister,                TF: 0.00766\n",
            "  Word: door,                  TF: 0.00736\n",
            "  Word: mother,                TF: 0.00683\n",
            "  Word: project,               TF: 0.00668\n",
            "  Word: ani,                   TF: 0.00637\n",
            "  Word: back,                  TF: 0.0063\n",
            "  Word: time,                  TF: 0.00562\n",
            "  Word: way,                   TF: 0.00501\n",
            "  Word: onli,                  TF: 0.00478\n",
            "  Word: look,                  TF: 0.00463\n",
            "  Word: one,                   TF: 0.00463\n",
            "  Word: nt,                    TF: 0.0044\n",
            "  Word: gutenbergtm,           TF: 0.00433\n",
            "  Word: open,                  TF: 0.00417\n",
            "  Word: use,                   TF: 0.00402\n",
            "  Word: get,                   TF: 0.00395\n",
            "  Word: said,                  TF: 0.00387\n",
            "  Word: littl,                 TF: 0.00372\n",
            "  Word: go,                    TF: 0.00372\n",
            "  Word: without,               TF: 0.00357\n",
            "  Word: still,                 TF: 0.00342\n",
            "  Word: first,                 TF: 0.00342\n",
            "  Word: want,                  TF: 0.00334\n",
            "  Word: see,                   TF: 0.00319\n",
            "  Word: like,                  TF: 0.00311\n",
            "  Word: hand,                  TF: 0.00311\n",
            "  Word: made,                  TF: 0.00304\n",
            "  Word: make,                  TF: 0.00304\n",
            "  Word: befor,                 TF: 0.00304\n",
            "  Word: much,                  TF: 0.00296\n",
            "  Word: head,                  TF: 0.00296\n",
            "  Word: come,                  TF: 0.00296\n",
            "  Word: move,                  TF: 0.00288\n",
            "  Word: day,                   TF: 0.00288\n",
            "  Word: thing,                 TF: 0.00288\n",
            "  Word: chief,                 TF: 0.00288\n",
            "  Word: clerk,                 TF: 0.00281\n",
            "  Word: thought,               TF: 0.00281\n",
            "  Word: turn,                  TF: 0.00273\n",
            "  Word: away,                  TF: 0.00266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8v__n0OigGKf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "52b3d73d-680b-49cd-999a-b2b079b00d13"
      },
      "cell_type": "code",
      "source": [
        "# Calculate tf scores for top 50 words from wordsWithoutPrivateStopWords token sets\n",
        "printTopTF(wordsWithoutPrivateStopWords, 50)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Word: gregor,                TF: 0.02739\n",
            "  Word: she,                   TF: 0.01838\n",
            "  Word: him,                   TF: 0.01728\n",
            "  Word: her,                   TF: 0.01719\n",
            "  Word: room,                  TF: 0.01222\n",
            "  Word: you,                   TF: 0.01204\n",
            "  Word: father,                TF: 0.00937\n",
            "  Word: sister,                TF: 0.00928\n",
            "  Word: door,                  TF: 0.00891\n",
            "  Word: mother,                TF: 0.00827\n",
            "  Word: project,               TF: 0.00809\n",
            "  Word: himself,               TF: 0.008\n",
            "  Word: i,                     TF: 0.0079\n",
            "  Word: back,                  TF: 0.00763\n",
            "  Word: look,                  TF: 0.00561\n",
            "  Word: nt,                    TF: 0.00533\n",
            "  Word: gutenbergtm,           TF: 0.00524\n",
            "  Word: open,                  TF: 0.00505\n",
            "  Word: get,                   TF: 0.00478\n",
            "  Word: just,                  TF: 0.00469\n",
            "  Word: said,                  TF: 0.00469\n",
            "  Word: littl,                 TF: 0.0045\n",
            "  Word: go,                    TF: 0.0045\n",
            "  Word: want,                  TF: 0.00404\n",
            "  Word: we,                    TF: 0.00395\n",
            "  Word: hand,                  TF: 0.00377\n",
            "  Word: head,                  TF: 0.00358\n",
            "  Word: again,                 TF: 0.00358\n",
            "  Word: move,                  TF: 0.00349\n",
            "  Word: thing,                 TF: 0.00349\n",
            "  Word: chief,                 TF: 0.00349\n",
            "  Word: clerk,                 TF: 0.0034\n",
            "  Word: thought,               TF: 0.0034\n",
            "  Word: turn,                  TF: 0.00331\n",
            "  Word: away,                  TF: 0.00322\n",
            "  Word: samsa,                 TF: 0.00312\n",
            "  Word: how,                   TF: 0.00303\n",
            "  Word: let,                   TF: 0.00303\n",
            "  Word: famili,                TF: 0.00294\n",
            "  Word: went,                  TF: 0.00294\n",
            "  Word: bed,                   TF: 0.00294\n",
            "  Word: came,                  TF: 0.00285\n",
            "  Word: seem,                  TF: 0.00285\n",
            "  Word: gutenberg,             TF: 0.00285\n",
            "  Word: left,                  TF: 0.00285\n",
            "  Word: soon,                  TF: 0.00285\n",
            "  Word: quit,                  TF: 0.00276\n",
            "  Word: too,                   TF: 0.00276\n",
            "  Word: everyth,               TF: 0.00267\n",
            "  Word: electron,              TF: 0.00267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RFoZFQCDaWh_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experimenting with using the [Gutenberg](https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html) dataset to compute our own stop word list\n",
        "Done by calculating the IDF wights for each word. This is dicussed in [this paper](http://terrierteam.dcs.gla.ac.uk/publications/rtlo_DIRpaper.pdf)\n",
        "\n",
        "Following the guide [here](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=KHeruhacFpSU) on how to download a file from google drive into the connected host machine\n",
        "\n",
        "**Result from downloading the gutenberg dataset is that the 1.2gb dataset is too big for downloading without being disconnected from the colab-host machine. Above are the results using a wikipedia dataset**  "
      ]
    },
    {
      "metadata": {
        "id": "qmvse5Nn4Fao",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Autheticate your google user and give google cloud SDK accessrights, which will enable copying file from your drive\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ePJHVxF94FXG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download the file we just uploaded.\n",
        "#\n",
        "# Replace the assignment below with your file ID\n",
        "# to download a different file.\n",
        "#\n",
        "# A file ID looks like: 1uBtlaggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '17WBziFbt9nhAW5iV-yHPHmCfquBPrjJO'\n",
        "\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "request = drive_service.files().get_media(fileId=file_id)\n",
        "downloaded = io.BytesIO()\n",
        "downloader = MediaIoBaseDownload(downloaded, request)\n",
        "done = False\n",
        "while done is False:\n",
        "  # _ is a placeholder for a progress object that we ignore.\n",
        "  # (Our file is small, so we skip reporting progress.)\n",
        "  _, done = downloader.next_chunk()\n",
        "\n",
        "downloaded.seek(0)\n",
        "print('Downloaded file contents are: {}'.format(downloaded.read()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aVkvKhUT2de_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# It is also a higly time efficient way of creating tf-idf weights, by using the sklearn package\n",
        "# this initializer with dummy functions allows the input to be a list of already tokenized texts in stead of a long string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def dummy_fun(doc):\n",
        "    return doc\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer='word',\n",
        "    tokenizer=dummy_fun,\n",
        "    preprocessor=dummy_fun,\n",
        "    token_pattern=None)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}